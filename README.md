## ðŸ”´ AI Red Teaming & LLM Security Research

I design and execute structured AI red-team exercises focused on identifying, documenting, and mitigating failure modes in LLM-powered systems.

My work emphasizes:
- Prompt-based adversarial testing (memory evasion, authority abuse, policy contradiction)
- Defense-in-depth policy enforcement
- Canonical denial design and statelessness guarantees
- Evidence-driven findings with mitigation plans and residual risk tracking

### Featured Project: AI Red Team Lab
A full end-to-end red-team exercise against a locally hosted LLM API, including threat modeling, adaptive adversarial testing, policy hardening, and post-mitigation validation.

**Key Capabilities Demonstrated**
- Stage-based red-team methodology (threat modeling â†’ evasion â†’ mitigation â†’ residual risk)
- Multi-turn adversarial pressure testing
- Training data claim suppression
- Implicit context and memory boundary enforcement
- Formal security documentation (findings, mitigations, residual risk)

**Repository:** https://github.com/rodneystanley2025/ai-red-team-lab
